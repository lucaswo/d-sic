{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting healy cruises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"http://icefloe.net/Aloftcon_Photos/albums/2015/20150809-2201.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = Image.open(io.BytesIO(r.content))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in range(0, 24):\n",
    "    x = str(x)\n",
    "    count = \"20110720-{}01\".format(\"0\"+x if len(x) < 2 else x)\n",
    "    \n",
    "    try:\n",
    "        with open('data/'+count+'.jpg', 'xb') as f:\n",
    "            img = requests.get(\"http://icefloe.net/Aloftcon_Photos/albums/2011/%s.jpeg\" % count)\n",
    "            \n",
    "            for chunk in img:\n",
    "                f.write(chunk)\n",
    "                \n",
    "        print(\"wrote\", count, \"to disk\")\n",
    "        time.sleep(1)\n",
    "    except FileExistsError:\n",
    "        print(\"file\", count, \"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations = pd.read_table(\"HLY1101_1min.r2rnav\", header=None, names=[\"time\", \"longitude\", \"latitude\", \"speed\", \"heading\"], index_col=0, skiprows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comp_re = re.compile(\"2011\\-07\\-20T\\d{2}\\:(01\\:00|00\\:59)\")\n",
    "locations[locations.index.str.contains(comp_re)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cruises = requests.get(\"http://www.rvdata.us/catalog/Healy\")\n",
    "cruises = BeautifulSoup(cruises.text, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cruises.select(\"a[href*=catalog/]\")[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = \"http://get.rvdata.us/cruise/{0}/products/r2rnav/{0}_1min.r2rnav\"\n",
    "comp_re = re.compile(\"T\\d{2}\\:(01\\:0[0-9]|00\\:5[0-9])\")\n",
    "locations = None\n",
    "i = 0\n",
    "\n",
    "for link in cruises.select(\"a[href*=catalog/]\"):\n",
    "    \n",
    "    # check if after 2009\n",
    "    if int(link.text[3:5]) >= 10:\n",
    "        print(base_url.format(link.text))\n",
    "        locations_tmp = pd.read_table(base_url.format(link.text), header=None, \n",
    "                                      names=[\"time\", \"longitude\", \"latitude\", \"speed\", \"heading\", \"image\"], \n",
    "                                      index_col=0, skiprows=3)\n",
    "        \n",
    "        if not locations_tmp.empty:\n",
    "            print(len(locations_tmp))\n",
    "            if locations is not None:\n",
    "                locations = locations.append(locations_tmp[locations_tmp.index.str.contains(comp_re)])\n",
    "            else:\n",
    "                locations = locations_tmp[locations_tmp.index.str.contains(comp_re)].copy()\n",
    "        \n",
    "        \n",
    "    \n",
    "    #if i == 0:\n",
    "    #    break\n",
    "        \n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "locations.index = pd.to_datetime(locations.index)\n",
    "locations[\"ice\"] = 254 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# direct download\n",
    "base_url_img = \"http://icefloe.net/Aloftcon_Photos/albums/{}/{}.jpeg\"\n",
    "\n",
    "for point in locations.index:\n",
    "\n",
    "    count = point.strftime(\"%Y%m%d-%H01\")\n",
    "\n",
    "    img = requests.get(base_url_img.format(point.year, count))\n",
    "    #print(\"http://icefloe.net/Aloftcon_Photos/albums/{}/{}.jpeg\".format(point.year, count))\n",
    "\n",
    "    locations.loc[point, \"image\"] = img.content\n",
    "    time.sleep(1)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare list of urls for download, add paths to DataFrame\n",
    "base_url_img = \"http://icefloe.net/Aloftcon_Photos/albums/{}/{}.jpeg\"\n",
    "base_path = \"data/images/remote/{}.jpeg\"\n",
    "\n",
    "with open(\"data/url_list_img\", \"w\") as url_list:\n",
    "    for point in locations.index:\n",
    "\n",
    "        count = point.strftime(\"%Y%m%d-%H01\")\n",
    "        \n",
    "        url_list.write(base_url_img.format(point.year, count)+ \"\\n\")\n",
    "        \n",
    "locations[\"image\"] = locations.index.map(lambda x: base_path.format(x.strftime(\"%Y%m%d-%H01\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locations[((locations['ice'] > 0) & (locations['ice'] <= 100)) | (locations['ice'] == 255) | (locations['ice'] == 252)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations.to_hdf(\"data/backup/complete_paths.h5\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations = pd.read_hdf(\"data/backup/complete_paths.h5\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice extent with Lon/Lat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = h5py.File(\"data/extent/remote/NISE_SSMISF17_20150908.h5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extent = tmp[\"Northern Hemisphere/Data Fields/Extent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def haversine_np(lons, lats, lon2, lat2):\n",
    "    lons, lats, lon2, lat2 = map(np.radians, [lons, lats, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lons\n",
    "    dlat = lat2 - lats\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lats) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    \n",
    "    idx = np.unravel_index(np.argmin(km), lats.shape)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lats = np.fromfile('data/extent/grid/EASE2_N25km.lats.720x720x1.double', dtype=np.float64).reshape((720,720))\n",
    "lons = np.fromfile('data/extent/grid/EASE2_N25km.lons.720x720x1.double', dtype=np.float64).reshape((720,720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations[\"ice\"] = locations.apply(lambda x: extent[haversine_np(lons, lats, x[\"longitude\"], x[\"latitude\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dates = np.unique(locations.index.date)\n",
    "base_url_extent = \"ftp://n5eil01u.ecs.nsidc.org/SAN/OTHR/NISE.004/{}/NISE_SSMISF17_{}.HDFEOS\"\n",
    "# ftp://n5eil01u.ecs.nsidc.org/SAN/OTHR/NISE.004/2015.08.09/NISE_SSMISF17_20150809.HDFEOS\n",
    "\n",
    "with open(\"data/download/url_list_extent\", \"w\") as url_list:\n",
    "    for date in dates:\n",
    "        folder = date.strftime(\"%Y.%m.%d\")\n",
    "        file = date.strftime(\"%Y%m%d\")\n",
    "        url_list.write(base_url_extent.format(folder, file)+\"\\n\")\n",
    "    \n",
    "    #locations.loc[locations.index.date == date, \"ice\"] = locations[locations.index.date == date].apply(lambda x: extent[haversine_np(lons, lats, x[\"longitude\"], x[\"latitude\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# missing values for 2012-02-01, nise_ssmisf17_20120201.h5\n",
    "locations = locations.drop(locations.index[locations.index.date == dates[364]])\n",
    "dates = np.unique(locations.index.date)\n",
    "\n",
    "for date in dates:\n",
    "    file = date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    extent = h5py.File(\"data/extent/remote/NISE_SSMISF17_{}.h5\".format(file), 'r')\n",
    "    extent = extent[\"Northern Hemisphere/Data Fields/Extent\"]\n",
    "    \n",
    "    locations.loc[locations.index.date == date, \"ice\"] = locations[locations.index.date == date].apply(lambda x: extent[haversine_np(lons, lats, x[\"longitude\"], x[\"latitude\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = np.dstack([lats.ravel(),lons.ravel()])[0].reshape(720,720,2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a[haversine_np(lons, lats, -166.477043, 53.924185)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
